import json
import numpy as np
import argparse


def run_page(graph, damping=0.85, epsilon=5.0e-8):
  inlinks = {}
  outlink_counts = {}
  inlink_counts = {}
  nodes = {}
  m = np.zeros(shape=(6667,6667))
  b = np.zeros(shape=(6667,6667))
  for node1 in graph:
    for node2 in graph[node1]:
      m[int(node1)][int(node2)] = 1.0;
  print 'created m'
  for x in range(0,6667):
    count = np.sum(m[x])
    if count == 0:
      for y in range(0,6667):
        m[x][y] = 1.0;
  print 'pages with no outgoing links are deemed to have outgoing links to all pages with equal probability'
  sum_col = m.sum(axis=0)
  for x in range(0,6667):
    if sum_col[x] == 0:
      for y in range(0,6667):
        m[y][x] = 1.0;
  print 'pages with no incoming links are deemed to have incoming links from all other pages with equal probability'
  for x in range(0, 6667):
    count = np.sum(m[x])
    for y in range(0,6667):
      b[x][y] = m[x][y]/count
  print'created b'

  ranks = np.zeros(shape=(6667,1))
  for x in range(0,6667):
    ranks[x][0] = 1.0/6667;
  b = np.transpose(b)
  print'starting compute'
  delta = 1
  while delta > epsilon:
    new_ranks = np.matmul(b, ranks)
    delta = np.sum(abs(new_ranks-ranks))
    #print delta
    #print ranks
    ranks = new_ranks
  return ranks

    



if __name__ == '__main__':
  arg_parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)

  arg_parser.add_argument('--adj_list', help='path to the adjacency list life (JSON)',
                type=str, default='./adj_list.json')

  arg_parser.add_argument('--url_dict', help='path to URL to ID mapping file (JSON)',
                type=str, default='./url_dict.json')

  arg_parser.add_argument('--url_dict_reverse', help='path to ID to URL mapping file (JSON)',
                type=str, default='./url_dict_reverse.json')

  arg_parser.add_argument('--output_path', help='path to the PAGERANK output file (will be generated by this script',
                type=str, default='./pagerank_scores.json')

  arg_parser.add_argument('--query_url', help='URL for which pagerank scores need to be output',
                type=str, default='none')

  arg_parser.add_argument('--k', help='number of pagerank (top-k) for which the scores should be printed',
                type=int, default=5)
#  arg_parser.add_argument('--', help='',
#                type=, default=)

  

  args = arg_parser.parse_args();
  
  adj_list = json.load(open(args.adj_list, 'r'))
  url_dict = json.load(open(args.url_dict, 'r'))
  url_dict_reverse = json.load(open(args.url_dict_reverse, 'r'))

  pg = run_page(adj_list)
  res = {}
  for x in range(0, 6667):
    res[str(x)] = pg[x][0]

  open(args.output_path, 'w').write(json.dumps(res))

  pr = [(url_dict_reverse[id_], res[id_]) for id_ in res]
  sorted_pr = sorted(pr, key=lambda x:x[1], reverse=True)
  for tup in sorted_pr[:args.k]:
    print(str(tup[1])+'\t'+tup[0])

  if args.query_url is not 'none':
    print('Query url: ' + args.query_url)
    if args.query_url in url_dict and url_dict[args.query_url] in res:
      print('page rank =' + str(round(res[url_dict[args.query_url]],7)))
    else:
      print 'page rank = 0'

  print 'pagerank stats: '
  max_hub = 0
  min_hub = 1
  median_hub = 0
  std_hub = 0
  i = 0
  hubs = np.ones(shape=(500))

  for tup in sorted_pr[:500]:
    if tup[1] > max_hub:
      max_hub = tup[1]
    if tup[1] < min_hub:
      min_hub = tup[1]
    if i == 249 or i == 250:
      median_hub+=tup[1]
    hubs[i] = tup[1]
    i+=1

  print('max = '+str(max_hub))
  print('min = '+str(min_hub))
  print('median = '+str(median_hub/2))
  print('std = '+str(np.std(hubs)))

 

